---
author: 宾狗
date: 2018-07-31 14:43+08:00
layout: post
title: "t-SNE使用过程中的一些坑"
description: ""
mathjax: true
categories:
- 学术
tags:
- 深度学习
---

* content
{:toc}

## 0x00 背景

几年前，我写过一个关于`t-SNE`原理的介绍博客，在日常的工作中，涉及到数据可视化的时候一般都会想到去使用这个工具。但是使用归使用，大部分人却很少去思考为什么要用`t-SNE`，怎样“正确”的使用`t-SNE`。有的同学可能会觉得奇怪，就一个可视化分析的方法而已，怎么还涉及到了用法的“正确”与“错误”了呢？事实上，正是因为很多人对`t-SNE`的细节不甚了解，将其他传统的可视化方法的认知套用在了`t-SNE`上，犯了错误还浑然不知，进而得出了一些看似正确合理的结论。





在刚刚结束的CVPR 2018上，`t-SNE`的原作者Laurens亲自出来做了一个tutorial，标题是`Do's and Don'ts of using t-SNE to Understand Vision Models`，里面列举了很多错误的使用`t-SNE`的范例，这里作一个简短的笔记分享，重温一下这个经典的方法，同时也加深对一些细节问题的理解。

## 0x01 基本原理

`t-SNE`本质上是基于流行学习(manifold learning)的降维算法，不同于传统的PCA和MMD等方法，`t-SNE`在高维用normalized Gaussian kernel对数据点对进行相似性建模。相应的，在低维用t分布对数据点对进行相似性(直观上的距离)建模，然后用KL距离来拉近高维和低维空间中的距离分布。这一块就不具体展开了，想深入了解的话，请回顾之前的文章[从SNE到t-SNE再到LargeVis](https://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/)

如果想对`t-SNE`的算法原理有更深入的认识，还是建议大家认真阅读一版`t-SNE`的实现的代码，很多模糊不清的地方看了代码之后自然就很明了，我推荐这个`Javascript`版本的[代码](https://github.com/karpathy/tsnejs)，逻辑结构非常清晰，我这里也列出一个算法流程图，辅助理解。

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/9e05e2da252fb4e915fc.png)


## 0x02 错误案例

下面我们就具体了解下`t-SNE`使用时有哪些坑，以及如何去避免犯这些错误。

### 可以用t-SNE来提出假设 不要用t-SNE得出结论

讲到这块的时候，Laurens说他参加顶会的时候喜欢四处逛逛，看到很多学术海报上数据可视化的部分用的正是`t-SNE`，虽然这个意味着又多了一个引用(偷笑)，但是很遗憾，有些论文的用法是错误的，是他所不希望看到的。

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/c67819b8907f4345675b.png)

比如他举了这样的一个例子，应该是NLP相关的论文，里面用`t-SNE`可视化了一些embedding出来，可以看到有一定类似语义迁移的规律，因此证明自己的方法是work的。但是很遗憾，这样做是错误的，因为如果把所有embedding同时乘以一个很大的数值，然后再用`t-SNE`做可视化，可以得到一个非常类似图。

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/87a184b829c86a45f882.png)

Laurens强调，可以通过`t-SNE`的可视化图提出一些假设，但是不要用`t-SNE`来得出一些结论，想要验证你的想法，最好用一些其他的办法。

### t-SNE中集群之间的距离并不表示相似度

这一块可以通过那个经典的MNIST可视化出来的效果图进行说明，如果所示：

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/0db2f0b939f5b2e39c53.png)

图中`0`和`1`的集群距离比较近，而`0`和`7`的集群距离较远，这说明`0`和`1`的相似度要更高吗？显然不是，事实上如果你在同一个数据上运行`t-SNE`算法多次，很有可能得到多个不同“形态”的集群，可能有的时候`0`和`1`集群比较近，可能`0`和`8`集群比较近。因此，考虑`t-SNE`可视化结果中不同集群之间的距离是没有意义的，因为对t分布来说，超出一定距离范围以后，其相似度都是很小的。也就是说，只要不在一个集群范围内，其相似度都是一个很小的值，我们所看到的集群之间的呈现出来的距离并不能说明什么，这是由`t-SNE`的内在所决定的。

### t-SNE不能用于寻找离群点outlier

这一点同样要回到原来的论文中去，`t-SNE`是在`SNE`的基础上改进而来的，其中一个改进就是把`SNE`改成了对称的形式，如下所示：

$$p_{ij}=\frac {p_{j|i}+p_{i|j}}{2N}$$

原来的条件概率建模和KL距离都是非对称的，而在t-SNE中加了一个对称项，相当于在某种程度上把outlier拉进了某个集群。为什么呢？我们考虑一个离群点和一个集群的情况，只要perplexity设置的合理，那么在选择近邻时，集群内的点显然不会选择离群点作为自己的邻居，这在非对称的条件下是没什么问题的。而在对称的条件下，我们还要额外考虑离群点选择近邻的情况，由于它自身是离群点，那么它只能选择离它最近的集群中的点作为近邻。加入了这一项之后，我们相当于无形之中拉近了集群和离群点之间的距离，所得到的结果是有偏差的。所以`t-SNE`不能用来寻找离群点。

### 别忘了scale(perplexity)的作用

大部分人在使用`t-SNE`时，一般都直接使用默认参数图个方便(一般perplexity的默认值是30)，如果忽视了perplexity带来的影响，有的时候遇到`t-SNE`可视化效果不好时，根本就不知道哪里出了问题，优化起来也就无从下手了。

那么perplexity到底是啥呢？我们可以回顾`t-SNE`的数学表达式，主要是和sigma这一项相关

perplexity表示了近邻的数量，例如设perplexity为2，那么就很有可能得到很多两个一对的小集群。

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/4e21e0131a33873f427c.png)

### t-SNE是在优化一个non-convex目标函数，只是局部最小

有的时候会出现同一集群被分为两半的情况，如下图所示

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/a6e29b6f8b6f30722e0e.png)

正如刚才所说的，`t-SNE`更关心的是学习维持局部结构，群间的距离并不能说明什么，而且每次跑`t-SNE`的结果并不完全一致。所以解决这个问题，我们只需要跑多次找出效果最好的就可以了。引起这个问题的本质原因是，`t-SNE`是在优化一个非凸的目标函数，我们每次得到的只不过是一个局部最小。

### 低维度量空间不能capture非度量的相似性，有些高维结构(距离 相似性)特征在低维是无法反映出来的

这部分Laurens列举了一个他经常用的例子，也就是下图中的几个雕塑：

![](http://lc-cf2bfs1v.cn-n1.lcfile.com/fe43cec22247459e7a2a.png)

最左侧的是一个`半人半马`形态的雕塑，背上骑着一个小孩，我们可以说它和右侧上方的骑兵雕塑相似(都是马上骑着一个人)，也可以说它和右下方的人雕塑相似(可能是人马的上半身和右下方的比较像)，但是我们不能说右上和右下的雕塑相似。

这个想要表达的意思是，`t-SNE`终究只是一个把高维空间数据映射到低维的可视化工具，它不能表征那些非metric的相似性。有些仅在高维空间中存在的相似性，在低维空间是没有办法表达出来的。


> t-SNE is a valuable tool in generating hypotheses and understanding, but does not produce conclusive evidence

## 0x03 其他资源

这个网站不仅做了`t-SNE`可视化的例子，还有CNN可解释性的例子，可视化效果做的非常棒，强烈建议大家去尝试一下

- https://distill.pub/2016/misread-tsne/